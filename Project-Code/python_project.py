# -*- coding: utf-8 -*-
"""Python-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YcNFYoAx-nJ_5bsAKZGobAA3bTHSS6lG

<h1 style="text-align:center">House Price Prediction Technique</h1>

# Step-1 Import Data
"""

import numpy as np 
import pandas as pd
pd.set_option('display.max_columns', None)
pd.set_option("max_rows", None)

from matplotlib import pyplot as plt
import matplotlib.pyplot as plte
import seaborn as sns
sns.set(style='whitegrid')

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.model_selection import RandomizedSearchCV

"""#Step-2 Exploring the Data"""

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv("../content/drive/My Drive/Python/train.csv")
test = pd.read_csv("../content/drive/My Drive/Python/test.csv")
sample_submission = pd.read_csv("../content/drive/My Drive/Python/sample_submission.csv")

train.head()

train.shape

test.head()

test.shape

"""##Step-3 Target Value: SalePrice"""

plt.figure(figsize=(20,10))
b = sns.distplot(train['SalePrice'])
b.set_title("SalePrice Distribution");

plt.figure(figsize=(20,10))
b = sns.boxplot(y = 'SalePrice', data = train)
b.set_title("SalePrice Distribution");

len(train[train['SalePrice'] > 700000])

"""We have two outliers where `SalePrice` is `> 700000`. Let's get rid of these."""

train.shape

train = train[train['SalePrice'] <= 700000]

train.shape

"""#Step-4 Handling missing values

**Where do we have NaN values?**
"""

train.columns[train.isna().any()].tolist()

test.columns[test.isna().any()].tolist()

"""Let's look at which values have what percentage of missing data:"""

#missing data
total = train.isnull().sum().sort_values(ascending=False)
percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

"""Let's delete the columns that have more than 15% missing data:"""

train = train.drop(['PoolQC'], axis=1)
test = test.drop(['PoolQC'], axis=1)

train = train.drop(['MiscFeature'], axis=1)
test = test.drop(['MiscFeature'], axis=1)

train = train.drop(['Alley'], axis=1)
test = test.drop(['Alley'], axis=1)

train = train.drop(['Fence'], axis=1)
test = test.drop(['Fence'], axis=1)

train = train.drop(['FireplaceQu'], axis=1)
test = test.drop(['FireplaceQu'], axis=1)

train = train.drop(['LotFrontage'], axis=1)
test = test.drop(['LotFrontage'], axis=1)

"""Let's first fill all numeric values with the median."""

train = train.fillna(train.median())
test = test.fillna(test.median())

"""Fill the other values with `None`."""

test['MSZoning'] = test['MSZoning'].fillna('None')

train = train.drop(['Utilities'], axis=1)
test = test.drop(['Utilities'], axis=1)

test['Exterior1st'] = test['Exterior1st'].fillna('None')

train.loc[train['Exterior1st'].value_counts()[train['Exterior1st']].values < 18,'Exterior1st'] = 'Rare'
test.loc[test['Exterior1st'].value_counts()[test['Exterior1st']].values < 18,'Exterior1st'] = 'Rare'

test['Exterior2nd'] = test['Exterior2nd'].fillna('None')

train.loc[train['Exterior2nd'].value_counts()[train['Exterior2nd']].values < 10,'Exterior2nd'] = 'Rare'
test.loc[test['Exterior2nd'].value_counts()[test['Exterior2nd']].values < 10,'Exterior2nd'] = 'Rare'

train['MasVnrType'] = train['MasVnrType'].fillna('Missing')
test['MasVnrType'] = test['MasVnrType'].fillna('Missing')

train['BsmtQual'] = train['BsmtQual'].fillna('None')
test['BsmtQual'] = test['BsmtQual'].fillna('None')

train['BsmtCond'] = train['BsmtCond'].fillna('None')
test['BsmtCond'] = test['BsmtCond'].fillna('None')

train['BsmtExposure'] = train['BsmtExposure'].fillna('None')
test['BsmtExposure'] = test['BsmtExposure'].fillna('None')

train['BsmtFinType1'] = train['BsmtFinType1'].fillna('None')
test['BsmtFinType1'] = test['BsmtFinType1'].fillna('None')

train['BsmtFinType2'] = train['BsmtFinType2'].fillna('None')
test['BsmtFinType2'] = test['BsmtFinType2'].fillna('None')

train['Electrical'] = train['Electrical'].fillna('None')
test['Electrical'] = test['Electrical'].fillna('None')

test['KitchenQual'] = test['KitchenQual'].fillna('None')

test['Functional'] = test['Functional'].fillna('None')

train['GarageType'] = train['GarageType'].fillna('None')
test['GarageType'] = test['GarageType'].fillna('None')

train['GarageFinish'] = train['GarageFinish'].fillna('None')
test['GarageFinish'] = test['GarageFinish'].fillna('None')

train['GarageQual'] = train['GarageQual'].fillna('None')
test['GarageQual'] = test['GarageQual'].fillna('None')

train['GarageCond'] = train['GarageCond'].fillna('None')
test['GarageCond'] = test['GarageCond'].fillna('None')

train['SaleType'] = train['SaleType'].fillna('None')
test['SaleType'] = test['SaleType'].fillna('None')

"""Let's check if we have any NaN values left:"""

train.isna().all().sum()

test.isna().all().sum()

"""Nope, we don't have any left!

#Step-5 Feature Engineering
"""

y_train = train['SalePrice'].values
df = pd.concat((train, test)).reset_index(drop=True)
df.drop(['SalePrice'], axis=1, inplace=True)

from sklearn.preprocessing import LabelEncoder
cols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 
        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', 
        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',
        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', 
        'YrSold', 'MoSold')
for c in cols:
    lbl = LabelEncoder() 
    lbl.fit(list(df[c].values)) 
    df[c] = lbl.transform(list(df[c].values))

# correlation heatmap
plt.figure(figsize=(25,25))
ax = sns.heatmap(train.corr(), cmap = "coolwarm", annot=True, linewidth=2)

# to fix the bug "first and last row cut in half of heatmap plot"
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5)

corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True);

df = pd.get_dummies(df)
print(df.shape)

"""#Step-6 Modeling"""

train = df[df['Id'] < 1461]
test = df[df['Id'] >= 1461]

X = train
y = y_train

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

# Put models in a dictionary
models = {"Lasso": Lasso(tol=0.1), 
          "RandomForestRegressor" : RandomForestRegressor(),
          "GradientBoostingRegressor" : GradientBoostingRegressor(),
          "XGBRegressor": XGBRegressor(),
          "LGBMRegressor": LGBMRegressor()
}

# Create function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):

    model_scores = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        model_scores[name] = np.sqrt(mean_squared_error(y_test, y_pred))
    return model_scores

model_scores = fit_and_score(models=models,
                             X_train=X_train,
                             X_test=X_test,
                             y_train=y_train,
                             y_test=y_test)
model_scores

gbr = GradientBoostingRegressor(n_estimators=5000)
gbr.fit(X_train, y_train)
y_pred = gbr.predict(X_test)
np.sqrt(mean_squared_error(y_test, y_pred))

"""#Step-7 Predicting values"""

y_pred = gbr.predict(test)
y_pred

sample_submission['SalePrice'] = y_pred
sample_submission.to_csv("submission.csv", index=False)
sample_submission.head()